{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBRw5QHhBkax"
   },
   "source": [
    "# 数据集导入(预制链接)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaD7ugivEL2R"
   },
   "source": [
    "## 官方版本数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfHjWRgKBR47",
    "outputId": "04e8d90d-2b4b-40b2-e428-bdfccc33899a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-14 08:11:01--  https://drive.usercontent.google.com/download?id=1JwR0Q1ArTg6c47EF2ZuIBpQwCPgXKrO2&export=download&authuser=0&confirm=t&uuid=dc3aa13c-c3a9-458f-983a-8586798cb635&at=APZUnTX25XMxi-z-3wBcgR93IGsL%3A1719235792953\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.253.118.132, 2404:6800:4003:c05::84\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.253.118.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1084182095 (1.0G) [application/octet-stream]\n",
      "Saving to: ‘Dataset.zip’\n",
      "\n",
      "Dataset.zip         100%[===================>]   1.01G  44.6MB/s    in 27s     \n",
      "\n",
      "2024-07-14 08:11:31 (38.1 MB/s) - ‘Dataset.zip’ saved [1084182095/1084182095]\n",
      "\n",
      "Archive:  Dataset.zip\n",
      "   creating: Dataset/\n",
      "   creating: Dataset/Testset_track_A/\n",
      "   creating: Dataset/Testset_track_A/Inference/\n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_658.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_659.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_660.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_662.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_663.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_664.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_665.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_666.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_667.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_668.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_672.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_673.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_674.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_675.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_676.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_677.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_678.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_679.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_681.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_683.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_684.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_686.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_687.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_688.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_689.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_690.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_691.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_692.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_693.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_695.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_696.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_697.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_700.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_701.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_702.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_703.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_704.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_705.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_708.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_709.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_710.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_711.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_712.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_713.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_715.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_717.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_718.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_719.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_721.ply  \n",
      "  inflating: Dataset/Testset_track_A/Inference/mesh_722.ply  \n",
      "   creating: Dataset/Testset_track_B/\n",
      "   creating: Dataset/Testset_track_B/Auxiliary/\n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_1.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_10.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_11.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_12.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_13.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_14.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_15.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_16.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_17.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_18.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_19.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_2.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_20.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_21.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_22.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_23.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_24.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_25.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_26.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_27.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_28.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_29.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_3.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_30.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_31.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_32.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_33.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_34.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_35.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_36.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_37.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_38.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_39.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_4.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_40.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_41.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_42.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_43.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_44.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_45.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_46.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_47.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_48.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_49.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_5.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_50.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_6.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_7.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_8.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_9.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/area_bounds.txt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/global_bounds.txt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_1.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_10.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_11.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_12.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_13.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_14.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_15.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_16.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_17.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_18.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_19.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_2.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_20.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_21.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_22.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_23.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_24.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_25.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_26.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_27.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_28.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_29.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_3.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_30.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_31.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_32.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_33.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_34.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_35.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_36.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_37.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_38.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_39.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_4.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_40.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_41.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_42.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_43.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_44.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_45.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_46.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_47.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_48.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_49.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_5.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_50.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_6.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_7.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_8.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_9.pt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/info_bounds.txt  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_1.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_10.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_11.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_12.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_13.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_14.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_15.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_16.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_17.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_18.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_19.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_2.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_20.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_21.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_22.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_23.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_24.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_25.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_26.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_27.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_28.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_29.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_3.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_30.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_31.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_32.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_33.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_34.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_35.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_36.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_37.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_38.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_39.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_4.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_40.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_41.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_42.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_43.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_44.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_45.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_46.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_47.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_48.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_49.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_5.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_50.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_6.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_7.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_8.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/normal_9.npy  \n",
      "  inflating: Dataset/Testset_track_B/Auxiliary/train_pressure_mean_std.txt  \n",
      "  inflating: Dataset/Testset_track_B/IJCAI_data_doc_v1.pdf  \n",
      "   creating: Dataset/Testset_track_B/Inference/\n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_1.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_10.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_11.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_12.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_13.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_14.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_15.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_16.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_17.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_18.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_19.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_2.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_20.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_21.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_22.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_23.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_24.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_25.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_26.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_27.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_28.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_29.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_3.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_30.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_31.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_32.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_33.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_34.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_35.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_36.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_37.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_38.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_39.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_4.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_40.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_41.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_42.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_43.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_44.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_45.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_46.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_47.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_48.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_49.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_5.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_50.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_6.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_7.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_8.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/centroid_9.npy  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_1.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_10.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_11.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_12.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_13.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_14.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_15.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_16.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_17.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_18.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_19.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_2.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_20.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_21.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_22.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_23.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_24.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_25.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_26.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_27.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_28.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_29.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_3.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_30.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_31.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_32.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_33.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_34.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_35.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_36.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_37.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_38.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_39.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_4.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_40.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_41.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_42.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_43.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_44.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_45.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_46.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_47.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_48.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_49.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_5.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_50.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_6.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_7.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_8.ply  \n",
      "  inflating: Dataset/Testset_track_B/Inference/mesh_9.ply  \n",
      "  inflating: Dataset/Testset_track_B/track_B_data_dict.xlsx  \n",
      "   creating: Dataset/Training_data/\n",
      "   creating: Dataset/Training_data/Feature/\n",
      "  inflating: Dataset/Training_data/Feature/mesh_001.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_002.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_004.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_005.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_006.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_007.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_008.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_010.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_012.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_013.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_017.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_018.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_021.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_022.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_023.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_025.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_026.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_027.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_028.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_029.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_030.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_031.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_032.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_034.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_035.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_039.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_040.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_043.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_044.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_045.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_046.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_047.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_048.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_049.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_050.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_051.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_052.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_054.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_055.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_056.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_058.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_059.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_060.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_061.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_062.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_063.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_064.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_065.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_067.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_069.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_070.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_071.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_072.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_073.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_074.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_075.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_076.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_077.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_078.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_079.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_080.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_081.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_083.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_084.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_085.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_086.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_087.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_088.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_090.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_091.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_092.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_094.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_095.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_096.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_097.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_100.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_101.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_102.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_105.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_106.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_107.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_109.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_110.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_111.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_112.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_113.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_114.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_115.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_116.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_117.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_118.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_119.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_120.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_121.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_123.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_124.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_125.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_126.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_127.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_128.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_129.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_130.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_131.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_133.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_134.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_136.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_137.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_138.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_139.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_140.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_141.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_142.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_143.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_144.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_145.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_146.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_147.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_148.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_149.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_150.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_151.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_152.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_153.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_155.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_156.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_157.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_158.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_159.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_160.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_161.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_162.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_163.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_165.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_166.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_170.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_172.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_173.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_175.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_176.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_177.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_178.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_179.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_180.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_181.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_182.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_183.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_184.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_186.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_190.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_191.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_192.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_193.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_195.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_196.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_198.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_199.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_200.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_201.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_202.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_203.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_205.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_207.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_210.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_211.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_212.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_213.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_214.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_215.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_217.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_219.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_220.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_221.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_222.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_223.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_224.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_225.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_227.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_228.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_229.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_230.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_231.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_232.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_233.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_234.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_235.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_236.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_237.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_241.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_243.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_244.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_245.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_246.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_247.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_248.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_249.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_251.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_252.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_253.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_255.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_257.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_258.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_259.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_260.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_261.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_262.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_263.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_264.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_266.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_267.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_268.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_269.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_271.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_272.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_273.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_274.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_275.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_276.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_277.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_278.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_279.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_280.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_281.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_282.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_283.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_285.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_286.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_289.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_290.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_291.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_292.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_293.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_294.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_295.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_296.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_297.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_298.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_299.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_300.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_301.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_302.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_304.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_305.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_306.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_308.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_309.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_310.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_311.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_312.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_313.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_314.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_315.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_319.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_320.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_321.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_322.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_323.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_324.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_325.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_327.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_328.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_329.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_331.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_332.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_333.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_334.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_335.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_337.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_338.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_339.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_340.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_341.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_344.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_345.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_347.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_348.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_349.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_350.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_352.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_353.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_354.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_355.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_356.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_357.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_358.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_360.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_362.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_364.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_365.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_366.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_367.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_369.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_371.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_372.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_373.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_374.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_375.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_376.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_378.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_379.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_380.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_381.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_384.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_385.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_389.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_392.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_393.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_397.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_398.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_399.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_401.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_402.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_403.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_404.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_405.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_407.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_408.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_410.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_412.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_413.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_414.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_415.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_417.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_418.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_419.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_420.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_422.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_424.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_425.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_427.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_430.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_431.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_433.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_435.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_436.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_437.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_439.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_440.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_443.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_444.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_446.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_447.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_448.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_449.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_450.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_451.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_452.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_453.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_454.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_455.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_456.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_457.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_459.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_460.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_462.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_463.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_464.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_465.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_466.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_467.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_468.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_469.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_470.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_472.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_473.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_474.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_475.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_476.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_478.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_479.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_480.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_482.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_483.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_486.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_487.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_488.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_490.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_493.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_494.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_495.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_496.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_497.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_498.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_499.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_501.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_502.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_503.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_504.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_505.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_507.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_508.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_509.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_511.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_512.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_513.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_514.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_515.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_516.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_518.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_519.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_521.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_522.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_523.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_524.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_525.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_527.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_529.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_530.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_532.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_533.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_536.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_538.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_539.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_540.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_542.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_543.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_545.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_547.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_548.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_549.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_550.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_551.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_552.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_553.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_554.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_555.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_560.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_561.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_562.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_564.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_565.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_566.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_567.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_568.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_569.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_572.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_573.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_574.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_576.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_577.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_579.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_581.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_582.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_583.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_584.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_587.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_588.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_589.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_591.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_593.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_594.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_595.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_596.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_597.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_598.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_600.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_602.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_604.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_608.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_610.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_611.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_612.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_613.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_615.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_616.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_617.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_618.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_620.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_621.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_622.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_623.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_625.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_626.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_627.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_628.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_629.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_630.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_631.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_632.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_633.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_634.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_635.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_636.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_638.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_639.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_640.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_641.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_642.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_643.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_644.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_645.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_646.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_647.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_648.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_649.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_651.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_652.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_654.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_655.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_656.ply  \n",
      "  inflating: Dataset/Training_data/Feature/mesh_657.ply  \n",
      "   creating: Dataset/Training_data/Label/\n",
      "  inflating: Dataset/Training_data/Label/press_001.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_002.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_004.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_005.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_006.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_007.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_008.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_010.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_012.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_013.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_017.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_018.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_021.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_022.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_023.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_025.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_026.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_027.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_028.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_029.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_030.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_031.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_032.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_034.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_035.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_039.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_040.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_043.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_044.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_045.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_046.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_047.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_048.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_049.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_050.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_051.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_052.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_054.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_055.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_056.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_058.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_059.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_060.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_061.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_062.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_063.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_064.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_065.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_067.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_069.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_070.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_071.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_072.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_073.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_074.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_075.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_076.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_077.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_078.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_079.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_080.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_081.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_083.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_084.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_085.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_086.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_087.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_088.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_090.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_091.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_092.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_094.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_095.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_096.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_097.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_100.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_101.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_102.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_105.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_106.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_107.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_109.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_110.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_111.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_112.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_113.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_114.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_115.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_116.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_117.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_118.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_119.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_120.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_121.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_123.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_124.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_125.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_126.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_127.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_128.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_129.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_130.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_131.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_133.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_134.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_136.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_137.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_138.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_139.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_140.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_141.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_142.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_143.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_144.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_145.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_146.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_147.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_148.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_149.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_150.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_151.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_152.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_153.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_155.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_156.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_157.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_158.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_159.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_160.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_161.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_162.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_163.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_165.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_166.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_170.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_172.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_173.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_175.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_176.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_177.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_178.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_179.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_180.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_181.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_182.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_183.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_184.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_186.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_190.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_191.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_192.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_193.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_195.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_196.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_198.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_199.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_200.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_201.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_202.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_203.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_205.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_207.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_210.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_211.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_212.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_213.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_214.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_215.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_217.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_219.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_220.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_221.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_222.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_223.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_224.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_225.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_227.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_228.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_229.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_230.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_231.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_232.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_233.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_234.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_235.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_236.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_237.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_241.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_243.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_244.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_245.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_246.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_247.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_248.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_249.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_251.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_252.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_253.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_255.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_257.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_258.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_259.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_260.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_261.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_262.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_263.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_264.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_266.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_267.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_268.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_269.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_271.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_272.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_273.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_274.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_275.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_276.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_277.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_278.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_279.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_280.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_281.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_282.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_283.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_285.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_286.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_289.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_290.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_291.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_292.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_293.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_294.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_295.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_296.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_297.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_298.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_299.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_300.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_301.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_302.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_304.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_305.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_306.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_308.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_309.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_310.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_311.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_312.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_313.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_314.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_315.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_319.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_320.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_321.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_322.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_323.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_324.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_325.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_327.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_328.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_329.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_331.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_332.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_333.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_334.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_335.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_337.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_338.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_339.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_340.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_341.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_344.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_345.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_347.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_348.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_349.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_350.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_352.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_353.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_354.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_355.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_356.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_357.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_358.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_360.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_362.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_364.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_365.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_366.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_367.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_369.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_371.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_372.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_373.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_374.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_375.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_376.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_378.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_379.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_380.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_381.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_384.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_385.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_389.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_392.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_393.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_397.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_398.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_399.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_401.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_402.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_403.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_404.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_405.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_407.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_408.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_410.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_412.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_413.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_414.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_415.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_417.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_418.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_419.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_420.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_422.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_424.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_425.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_427.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_430.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_431.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_433.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_435.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_436.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_437.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_439.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_440.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_443.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_444.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_446.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_447.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_448.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_449.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_450.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_451.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_452.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_453.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_454.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_455.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_456.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_457.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_459.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_460.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_462.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_463.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_464.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_465.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_466.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_467.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_468.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_469.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_470.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_472.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_473.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_474.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_475.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_476.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_478.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_479.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_480.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_482.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_483.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_486.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_487.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_488.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_490.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_493.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_494.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_495.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_496.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_497.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_498.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_499.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_501.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_502.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_503.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_504.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_505.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_507.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_508.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_509.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_511.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_512.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_513.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_514.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_515.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_516.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_518.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_519.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_521.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_522.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_523.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_524.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_525.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_527.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_529.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_530.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_532.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_533.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_536.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_538.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_539.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_540.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_542.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_543.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_545.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_547.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_548.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_549.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_550.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_551.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_552.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_553.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_554.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_555.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_560.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_561.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_562.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_564.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_565.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_566.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_567.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_568.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_569.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_572.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_573.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_574.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_576.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_577.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_579.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_581.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_582.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_583.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_584.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_587.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_588.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_589.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_591.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_593.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_594.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_595.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_596.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_597.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_598.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_600.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_602.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_604.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_608.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_610.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_611.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_612.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_613.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_615.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_616.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_617.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_618.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_620.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_621.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_622.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_623.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_625.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_626.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_627.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_628.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_629.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_630.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_631.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_632.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_633.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_634.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_635.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_636.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_638.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_639.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_640.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_641.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_642.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_643.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_644.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_645.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_646.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_647.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_648.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_649.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_651.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_652.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_654.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_655.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_656.npy  \n",
      "  inflating: Dataset/Training_data/Label/press_657.npy  \n",
      "  inflating: Dataset/Training_data/train_pressure_min_std.txt  \n",
      "  inflating: Dataset/Training_data/watertight_global_bounds.txt  \n",
      "  inflating: Dataset/Training_data/watertight_meshes.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Cookie: __Secure-ENID=12.SE=Yd0Bj-CLJ14fnd4qzdJHmwUs4B5zz46UaPC1cPJigNqqFV9PtM2CYyBpSbCkOyzUwzlEdZ1nZFf-igtGi7wSdJ_gqQSfQfh84r9egqFQAy9-GKayCRbdQKdera-2mkpuIT-c64CyR9vfNojM3hxZ9Dej-dGvtxlGjal9ttEHybw; __gsas=ID=ae0421b9a34b478c:T=1710758437:RT=1710758437:S=ALNI_MZP13R9ZOHbCzC0rgHSMrGXj6GCsg; HSID=A-4I-ZudDNUIB6EKH; SSID=A7v_1v9un6xAwVNku; APISID=ctK8IbLjeuDUmgys/AFnMSLWt9KddceDI6; SAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; __Secure-1PAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; __Secure-3PAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; SID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_kzuBV1TvOhAIC8VF1e9fpgACgYKATQSARQSFQHGX2Mi8LXUwWoIwNCEPU8Sy3mXUxoVAUF8yKqGXVfjTGz9gQal7nwGr4Pl0076; __Secure-1PSID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_PDa-DzVmbdGFPyxMQpk9_QACgYKAewSARQSFQHGX2MiAeee4fn0OWglWZfAygqkyBoVAUF8yKp-Sfmtnueimxc-0QbJRF9I0076; __Secure-3PSID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_g9IrMeU98APBo9Stp6wEnAACgYKAQASARQSFQHGX2MiFWtc9ucONXnpxBzlRdudEhoVAUF8yKoeZwCpJDnjfAFjGssHSUGm0076; NID=515=GQhY9nKKFCx3qFDjE0MA4ubjWNdef6xCIY_RfWOPWKEtyfBN3nAUl8WHI2VczjNQ4rVkj1XBAY8WNWHXyqSK10CfT4FxsFlPzrHIJpeTtm1nWRNBd9AAfBKJHz4XpESszntVUTE_59RklZuKo0vk1poReVi2da1PZKC3CTKH2Ll3gB5xuB9wf4bmq8ylVUuIROPJczr0XnCuUHV3qLdBvgy9_870b6UwOq1iOlIxFQFm01EZ4pqF4q1Ub3QRSWpEMLh4LSZFpJ5O255R5OV7krmEdDvH_sHoTEPZAg2PoEpwAyGK6Xp9qcLIlldgx5-5V86N8Wtb93uTlQuA_CFXb5_2eP3bgeX8txwlJ5SrldVjg9ctzYtBU2RwJKTSvdHfIG7lpOkg6XlkvDOcJpR3DihT_OlqnPn7drCAJpvVDv29hZn5XPMXaSrNdbG64OJ9urJEw5odEwsLYkkpC1vmlUcuoo52S5f6RQu0Z8kZiV8iRW6XIqHsSmQHunVaxk6xWCStUg; __Secure-1PSIDTS=sidts-CjEB3EgAEtTS0OazynCofIH4RCBstiRP5flEcvYW3z4Fg9oGd5QOESDOZt1wO2iqUYHjEAA; __Secure-3PSIDTS=sidts-CjEB3EgAEtTS0OazynCofIH4RCBstiRP5flEcvYW3z4Fg9oGd5QOESDOZt1wO2iqUYHjEAA; SIDCC=AKEyXzVI6aMX8lSDja86Yts3FBAtBzPCzVNgaX5BCz78NWsWzlT3yFWKUV7ZE46SFzE1GiBI-cHdTw; __Secure-1PSIDCC=AKEyXzUo4NQAwqqPMxP2eye-MFEbZmBIm_sZqRU1amttg0YoQkc8ZKSNXdHl5jNCMEbhrUHhS9-K; __Secure-3PSIDCC=AKEyXzWf2lIdmDLeZKpXSi9GytVQb6XudrYiNUBA5gW952YuLh8kL6T3IbBlu8zOTfGEcdUp5O1R\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1JwR0Q1ArTg6c47EF2ZuIBpQwCPgXKrO2&export=download&authuser=0&confirm=t&uuid=dc3aa13c-c3a9-458f-983a-8586798cb635&at=APZUnTX25XMxi-z-3wBcgR93IGsL%3A1719235792953\" -c -O 'Dataset.zip'\n",
    "!unzip Dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITzT8s2wgZG0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-xXUEozBUJVZ"
   },
   "outputs": [],
   "source": [
    "!mkdir Dataset/data_test_A/\n",
    "!mv Dataset/Testset_track_A/Inference/* Dataset/data_test_A/\n",
    "!mv watertight_meshes.txt Dataset/data_test_A/\n",
    "!pip install plyfile -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOvVhsX6VtWy",
    "outputId": "0a8bf5a1-404c-47ae-be73-76addbe8feb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MTKXLAnp9Kdx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "path='data/'\n",
    "version='L4090-A004'\n",
    "\n",
    "os.makedirs(path+'models',exist_ok=True)\n",
    "os.makedirs(path+'feature',exist_ok=True)\n",
    "os.makedirs(path+'feature_importance',exist_ok=True)\n",
    "os.makedirs(path+'submissions',exist_ok=True)\n",
    "os.makedirs(path+'submissions/content/gen_answer_A/',exist_ok=True)\n",
    "os.makedirs(path+'logs',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mqL2U0-JT09Z"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "yL4zfytyT3AT",
    "outputId": "69aec662-cee8-436d-b1b6-10f932be9cb2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"470\",\n          \"096\",\n          \"488\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "train_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-069f7bce-dd5b-4d8e-a082-4636adcb71ce\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-069f7bce-dd5b-4d8e-a082-4636adcb71ce')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-069f7bce-dd5b-4d8e-a082-4636adcb71ce button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-069f7bce-dd5b-4d8e-a082-4636adcb71ce');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-7fb64c29-8e20-4eed-ba47-8c150b337ff7\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7fb64c29-8e20-4eed-ba47-8c150b337ff7')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-7fb64c29-8e20-4eed-ba47-8c150b337ff7 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_bc325ba6-bda3-4b36-bae5-084ebab6a46b\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_bc325ba6-bda3-4b36-bae5-084ebab6a46b button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('train_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      id\n",
       "0    001\n",
       "1    002\n",
       "2    004\n",
       "3    005\n",
       "4    006\n",
       "..   ...\n",
       "495  652\n",
       "496  654\n",
       "497  655\n",
       "498  656\n",
       "499  657\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv(\"Dataset/Training_data/watertight_meshes.txt\",header=None,dtype=str)\n",
    "train_df.columns=['id']\n",
    "train_df=train_df.sort_values(by=['id'],ascending=[True]).reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "4Q00sChYT3um",
    "outputId": "052df63d-65c5-4dea-b96a-59c1b11251f7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"675\",\n          \"709\",\n          \"696\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "test_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-4309ce0e-51bc-4967-96ab-b769b19decd1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4309ce0e-51bc-4967-96ab-b769b19decd1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-4309ce0e-51bc-4967-96ab-b769b19decd1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-4309ce0e-51bc-4967-96ab-b769b19decd1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-3c3a025c-e380-4518-94ee-8b13355166e0\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c3a025c-e380-4518-94ee-8b13355166e0')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-3c3a025c-e380-4518-94ee-8b13355166e0 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_7525c81c-e30f-429c-a42e-a91a0e4b146c\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_7525c81c-e30f-429c-a42e-a91a0e4b146c button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('test_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     id\n",
       "0   658\n",
       "1   659\n",
       "2   660\n",
       "3   662\n",
       "4   663\n",
       "5   664\n",
       "6   665\n",
       "7   666\n",
       "8   667\n",
       "9   668\n",
       "10  672\n",
       "11  673\n",
       "12  674\n",
       "13  675\n",
       "14  676\n",
       "15  677\n",
       "16  678\n",
       "17  679\n",
       "18  681\n",
       "19  683\n",
       "20  684\n",
       "21  686\n",
       "22  687\n",
       "23  688\n",
       "24  689\n",
       "25  690\n",
       "26  691\n",
       "27  692\n",
       "28  693\n",
       "29  695\n",
       "30  696\n",
       "31  697\n",
       "32  700\n",
       "33  701\n",
       "34  702\n",
       "35  703\n",
       "36  704\n",
       "37  705\n",
       "38  708\n",
       "39  709\n",
       "40  710\n",
       "41  711\n",
       "42  712\n",
       "43  713\n",
       "44  715\n",
       "45  717\n",
       "46  718\n",
       "47  719\n",
       "48  721\n",
       "49  722"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=pd.read_csv(\"Dataset/data_test_A/watertight_meshes.txt\",header=None,dtype=str)\n",
    "test_df.columns=['id']\n",
    "test_df=test_df.sort_values(by=['id'],ascending=[True]).reset_index(drop=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GXtZpOQT38h",
    "outputId": "d8c9a744-99ff-477c-f37c-3201d9fda9e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:29<00:00, 16.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from plyfile import PlyData\n",
    "from tqdm import tqdm\n",
    "train_pos=[]\n",
    "train_press=[]\n",
    "for file_id in tqdm(train_df['id']):\n",
    "    ply = PlyData.read(f\"Dataset/Training_data/Feature/mesh_{file_id}.ply\")\n",
    "    vtx = ply['vertex']\n",
    "    x=np.array(vtx['x']).reshape(1,3586,1)\n",
    "    y=np.array(vtx['y']).reshape(1,3586,1)\n",
    "    z=np.array(vtx['z']).reshape(1,3586,1)\n",
    "    pos=np.concatenate([x,y,z],axis=-1)\n",
    "    train_pos.append(pos)\n",
    "    press = np.load(f\"Dataset/Training_data/Label/press_{file_id}.npy\").reshape((-1,))\n",
    "    press = np.concatenate((press[0:16], press[112:]), axis=0).reshape(1,-1)\n",
    "    train_press.append(press)\n",
    "train_pos=np.concatenate(train_pos,axis=0).astype(np.float32)\n",
    "train_press=np.concatenate(train_press,axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YaBLGq0UtyV",
    "outputId": "1fbf83b0-566a-4b06-9631-ad74644a3c8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.21it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pos=[]\n",
    "test_press=[]\n",
    "for file_id in tqdm(test_df['id']):\n",
    "    ply = PlyData.read(f\"Dataset/data_test_A/mesh_{file_id}.ply\")\n",
    "    vtx = ply['vertex']\n",
    "    x=np.array(vtx['x']).reshape(1,3586,1)\n",
    "    y=np.array(vtx['y']).reshape(1,3586,1)\n",
    "    z=np.array(vtx['z']).reshape(1,3586,1)\n",
    "    pos=np.concatenate([x,y,z],axis=-1)\n",
    "    test_pos.append(pos)\n",
    "test_pos=np.concatenate(test_pos,axis=0).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5ci86pmAbrM",
    "outputId": "b443fa3d-2903-4002-9105-bb14a4738fdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-37.09, 48.0955)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,std=(-37.09, 48.0955)\n",
    "mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IR-oWQomArX8"
   },
   "outputs": [],
   "source": [
    "train_press=(train_press-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SkTWdadl8tXh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, Input1 , labels):\n",
    "        self.Input1 = torch.tensor(Input1)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Input1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.Input1[index],self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEmc5uvUEdji",
    "outputId": "fd16b6d2-8c25-43aa-d73a-3e538f1097f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-a55a3d3a4c85>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "test_dataset=MyDataset(test_pos,torch.zeros(test_pos.shape[0],train_press.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Dzv1EoeYDcL3"
   },
   "outputs": [],
   "source": [
    "test_loader=DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"PyTorch BERT model.\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    NextSentencePredictorOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n",
    "_CONFIG_FOR_DOC = \"BertConfig\"\n",
    "\n",
    "# TokenClassification docstring\n",
    "_CHECKPOINT_FOR_TOKEN_CLASSIFICATION = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "_TOKEN_CLASS_EXPECTED_OUTPUT = (\n",
    "    \"['O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'] \"\n",
    ")\n",
    "_TOKEN_CLASS_EXPECTED_LOSS = 0.01\n",
    "\n",
    "# QuestionAnswering docstring\n",
    "_CHECKPOINT_FOR_QA = \"deepset/bert-base-cased-squad2\"\n",
    "_QA_EXPECTED_OUTPUT = \"'a nice puppet'\"\n",
    "_QA_EXPECTED_LOSS = 7.41\n",
    "_QA_TARGET_START_INDEX = 14\n",
    "_QA_TARGET_END_INDEX = 15\n",
    "\n",
    "# SequenceClassification docstring\n",
    "_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"textattack/bert-base-uncased-yelp-polarity\"\n",
    "_SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_1'\"\n",
    "_SEQ_CLASS_EXPECTED_LOSS = 0.01\n",
    "\n",
    "\n",
    "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"bert-large-uncased\",\n",
    "    \"bert-base-cased\",\n",
    "    \"bert-large-cased\",\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"bert-base-chinese\",\n",
    "    \"bert-base-german-cased\",\n",
    "    \"bert-large-uncased-whole-word-masking\",\n",
    "    \"bert-large-cased-whole-word-masking\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-base-cased-finetuned-mrpc\",\n",
    "    \"bert-base-german-dbmdz-cased\",\n",
    "    \"bert-base-german-dbmdz-uncased\",\n",
    "    \"cl-tohoku/bert-base-japanese\",\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "    \"cl-tohoku/bert-base-japanese-char\",\n",
    "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\",\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n",
    "    \"wietsedv/bert-base-dutch-cased\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "]\n",
    "\n",
    "\n",
    "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
    "    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n",
    "    try:\n",
    "        import re\n",
    "\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        logger.error(\n",
    "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
    "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "        )\n",
    "        raise\n",
    "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n",
    "    # Load weights from TF model\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    names = []\n",
    "    arrays = []\n",
    "    for name, shape in init_vars:\n",
    "        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n",
    "        array = tf.train.load_variable(tf_path, name)\n",
    "        names.append(name)\n",
    "        arrays.append(array)\n",
    "\n",
    "    for name, array in zip(names, arrays):\n",
    "        name = name.split(\"/\")\n",
    "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "        # which are not required for using pretrained model\n",
    "        if any(\n",
    "                n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
    "                for n in name\n",
    "        ):\n",
    "            logger.info(f\"Skipping {'/'.join(name)}\")\n",
    "            continue\n",
    "        pointer = model\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
    "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
    "            else:\n",
    "                scope_names = [m_name]\n",
    "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
    "                pointer = getattr(pointer, \"bias\")\n",
    "            elif scope_names[0] == \"output_weights\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"squad\":\n",
    "                pointer = getattr(pointer, \"classifier\")\n",
    "            else:\n",
    "                try:\n",
    "                    pointer = getattr(pointer, scope_names[0])\n",
    "                except AttributeError:\n",
    "                    logger.info(f\"Skipping {'/'.join(name)}\")\n",
    "                    continue\n",
    "            if len(scope_names) >= 2:\n",
    "                num = int(scope_names[1])\n",
    "                pointer = pointer[num]\n",
    "        if m_name[-11:] == \"_embeddings\":\n",
    "            pointer = getattr(pointer, \"weight\")\n",
    "        elif m_name == \"kernel\":\n",
    "            array = np.transpose(array)\n",
    "        try:\n",
    "            if pointer.shape != array.shape:\n",
    "                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n",
    "        except ValueError as e:\n",
    "            e.args += (pointer.shape, array.shape)\n",
    "            raise\n",
    "        logger.info(f\"Initialize PyTorch weight {name}\")\n",
    "        pointer.data = torch.from_numpy(array)\n",
    "    return model\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            position_ids: Optional[torch.LongTensor] = None,\n",
    "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "            past_key_values_length: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "        # issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module) :\n",
    "    def __init__(self, config, position_embedding_type=None) :\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\") :\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\" :\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor :\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor] :\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None :\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention :\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None :\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else :\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        if self.training :\n",
    "            drop_rate = self.dropout.p\n",
    "        else :\n",
    "            drop_rate = 0\n",
    "        context_layer = F.scaled_dot_product_attention(query_layer, key_layer, value_layer,attn_mask=attention_mask,\n",
    "                                                       dropout_p=drop_rate).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        use_cache = past_key_value is not None\n",
    "        # if self.is_decoder: #默认为False\n",
    "        #     # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "        #     # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "        #     # key/value_states (first \"if\" case)\n",
    "        #     # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "        #     # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "        #     # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "        #     # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "        #     past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        # attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        #\n",
    "        #\n",
    "        # if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "        #     query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
    "        #     if use_cache:\n",
    "        #         position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
    "        #             -1, 1\n",
    "        #         )\n",
    "        #     else:\n",
    "        #         position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "        #     position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "        #     distance = position_ids_l - position_ids_r\n",
    "        #\n",
    "        #     positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "        #     positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "        #\n",
    "        #     if self.position_embedding_type == \"relative_key\":\n",
    "        #         relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "        #         attention_scores = attention_scores + relative_position_scores\n",
    "        #     elif self.position_embedding_type == \"relative_key_query\":\n",
    "        #         relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "        #         relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "        #         attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "        #\n",
    "        # attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # if attention_mask is not None:\n",
    "        #     # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        #     attention_scores = attention_scores + attention_mask.to(torch.float16)\n",
    "        #\n",
    "        # # Normalize the attention scores to probabilities.\n",
    "        # attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        # attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        # if head_mask is not None:\n",
    "        #     attention_probs = attention_probs * head_mask\n",
    "\n",
    "        # context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        # outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        outputs = (context_layer,)\n",
    "\n",
    "        if self.is_decoder :\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n",
    "                    \" by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "            output_hidden_states: Optional[bool] = False,\n",
    "            return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class BertOnlyNSPHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score\n",
    "\n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BertForPreTrainingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output type of [`BertForPreTraining`].\n",
    "\n",
    "    Args:\n",
    "        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "BERT_START_DOCSTRING = r\"\"\"\n",
    "\n",
    "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
    "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
    "    etc.)\n",
    "\n",
    "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
    "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
    "    and behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`BertConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "BERT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `({0})`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
    "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
    "            1]`:\n",
    "\n",
    "            - 0 corresponds to a *sentence A* token,\n",
    "            - 1 corresponds to a *sentence B* token.\n",
    "\n",
    "            [What are token type IDs?](../glossary#token-type-ids)\n",
    "        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n",
    "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
    "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
    "            model's internal embedding lookup matrix.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
    "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n",
    "    sentence prediction (classification)` head.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            next_sentence_label: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BertForPreTrainingOutput]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n",
    "                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n",
    "                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n",
    "\n",
    "                - 0 indicates sequence B is a continuation of sequence A,\n",
    "                - 1 indicates sequence B is a random sequence.\n",
    "            kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n",
    "                Used to hide legacy arguments that have been deprecated.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, BertForPreTraining\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "\n",
    "        >>> prediction_logits = outputs.prediction_logits\n",
    "        >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BERT_START_DOCSTRING\n",
    ")\n",
    "class BertLMHeadModel(BertPreTrainedModel):\n",
    "    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if not config.is_decoder:\n",
    "            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=CausalLMOutputWithCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            past_key_values: Optional[List[torch.Tensor]] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
    "            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
    "            ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if labels is not None:\n",
    "            use_cache = False\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
    "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "            self, input_ids, past_key_values=None, attention_mask=None, use_cache=True, **model_kwargs\n",
    "    ):\n",
    "        input_shape = input_ids.shape\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "        # cut decoder_input_ids if past_key_values is used\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    def _reorder_cache(self, past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top.\"\"\", BERT_START_DOCSTRING)\n",
    "class BertForMaskedLM(BertPreTrainedModel):\n",
    "    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MaskedLMOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "        expected_output=\"'paris'\",\n",
    "        expected_loss=0.88,\n",
    "    )\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n",
    "            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        effective_batch_size = input_shape[0]\n",
    "\n",
    "        #  add a dummy token\n",
    "        if self.config.pad_token_id is None:\n",
    "            raise ValueError(\"The PAD token should be defined for generation\")\n",
    "\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n",
    "        dummy_token = torch.full(\n",
    "            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top.\"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyNSPHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "            **kwargs,\n",
    "    ) -> Union[Tuple[torch.Tensor], NextSentencePredictorOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see `input_ids` docstring). Indices should be in `[0, 1]`:\n",
    "\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, BertForNextSentencePrediction\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "        >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "        >>> encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n",
    "\n",
    "        >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "        >>> logits = outputs.logits\n",
    "        >>> assert logits[0, 0] < logits[0, 1]  # next sentence was random\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        if \"next_sentence_label\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use\"\n",
    "                \" `labels` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"next_sentence_label\")\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        seq_relationship_scores = self.cls(pooled_output)\n",
    "\n",
    "        next_sentence_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (seq_relationship_scores,) + outputs[2:]\n",
    "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n",
    "\n",
    "        return NextSentencePredictorOutput(\n",
    "            loss=next_sentence_loss,\n",
    "            logits=seq_relationship_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
    "    output) e.g. for GLUE tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n",
    "        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n",
    "    )\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
    "    softmax) e.g. for RocStories/SWAG tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForMultipleChoice(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n",
    "            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n",
    "            `input_ids` above)\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
    "        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
    "        inputs_embeds = (\n",
    "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
    "            if inputs_embeds is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        reshaped_logits = logits.view(-1, num_choices)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(reshaped_logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (reshaped_logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return MultipleChoiceModelOutput(\n",
    "            loss=loss,\n",
    "            logits=reshaped_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
    "    Named-Entity-Recognition (NER) tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n",
    "        output_type=TokenClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n",
    "        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n",
    "    )\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
    "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_QA,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "        qa_target_start_index=_QA_TARGET_START_INDEX,\n",
    "        qa_target_end_index=_QA_TARGET_END_INDEX,\n",
    "        expected_output=_QA_EXPECTED_OUTPUT,\n",
    "        expected_loss=_QA_EXPECTED_LOSS,\n",
    "    )\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            start_positions: Optional[torch.Tensor] = None,\n",
    "            end_positions: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n",
    "        r\"\"\"\n",
    "        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
    "            are not taken into account for computing the loss.\n",
    "        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
    "            are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XLNZMxywPYe1"
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "# from modeling_bert_SDPA import BertModel\n",
    "from torch import nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.config = BertConfig()\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.config.hidden_size = 512\n",
    "        self.config.num_attention_heads = 4\n",
    "        self.config.num_hidden_layers = 12\n",
    "        self.config.max_position_embeddings=3586\n",
    "        self.config.intermediate_size=2048\n",
    "        self.config.vocab_size=1\n",
    "        self.bert = BertModel(self.config)\n",
    "        self.fc1=nn.Linear(3,self.config.hidden_size)\n",
    "        self.fc2=nn.Linear(self.config.hidden_size,1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.fc1(x)\n",
    "        output = self.bert(inputs_embeds=x)[0]\n",
    "        output=self.fc2(output).squeeze(dim=-1)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PK_IRye3DV0u"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_R4ymI9BcKYb",
    "outputId": "70dccd43-3da1-4a5e-fcd3-24fee2fbc64a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from loguru import logger\n",
    "\n",
    "logger.add(path+f\"logs/log_{version}.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4Xrju_8XWMkx"
   },
   "outputs": [],
   "source": [
    "class LpLoss(nn.Module):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction='mean'):\n",
    "        super(LpLoss, self).__init__()\n",
    "        # Dimension and Lp-norm type are positive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Compute L2 norm of the difference and Lp norm of y\n",
    "        diff_norms = torch.norm(x - y, 2)\n",
    "        y_norms = torch.norm(y, self.p)\n",
    "\n",
    "        # Handle the reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(diff_norms / y_norms)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(diff_norms / y_norms)\n",
    "        else:\n",
    "            return diff_norms / y_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WxfeoTriWNRk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class valid_LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(valid_LpLoss, self).__init__()\n",
    "        # Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        diff_norms = np.linalg.norm(x-y, 2)\n",
    "        y_norms = np.linalg.norm(y, self.p)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return np.mean(diff_norms / y_norms)\n",
    "            else:\n",
    "                return np.sum(diff_norms / y_norms)\n",
    "\n",
    "        return diff_norms / y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-lNF7h-WNYC",
    "outputId": "8d99643d-47df-4af9-c804-d1601c61d877"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-14 08:29:00.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mFold [0] | Epoch [1/1] | Loss: 1.2585 | Valid loss: 0.9888 | Valid mse: 2268.85156 | Valid rmse: 47.63246 | Valid mae: 29.15616 | Valid score: 0.77940 | time: 210.5026\u001b[0m\n",
      "\u001b[32m2024-07-14 08:29:00.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mmodel saved\u001b[0m\n",
      "\u001b[32m2024-07-14 08:31:41.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mFold [1] | Epoch [1/1] | Loss: 1.0353 | Valid loss: 1.0773 | Valid mse: 2722.98877 | Valid rmse: 52.18226 | Valid mae: 38.71878 | Valid score: 0.85040 | time: 159.1661\u001b[0m\n",
      "\u001b[32m2024-07-14 08:31:41.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mmodel saved\u001b[0m\n",
      "\u001b[32m2024-07-14 08:34:15.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mFold [2] | Epoch [1/1] | Loss: 1.1739 | Valid loss: 1.1460 | Valid mse: 3020.66479 | Valid rmse: 54.96057 | Valid mae: 39.98763 | Valid score: 0.90655 | time: 153.1172\u001b[0m\n",
      "\u001b[32m2024-07-14 08:34:15.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mmodel saved\u001b[0m\n",
      "\u001b[32m2024-07-14 08:36:51.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mFold [3] | Epoch [1/1] | Loss: 1.0872 | Valid loss: 0.9784 | Valid mse: 2187.10352 | Valid rmse: 46.76648 | Valid mae: 27.94411 | Valid score: 0.77732 | time: 154.4222\u001b[0m\n",
      "\u001b[32m2024-07-14 08:36:51.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mmodel saved\u001b[0m\n",
      "\u001b[32m2024-07-14 08:39:26.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mFold [4] | Epoch [1/1] | Loss: 1.1184 | Valid loss: 1.0743 | Valid mse: 2691.44824 | Valid rmse: 51.87917 | Valid mae: 34.32491 | Valid score: 0.85481 | time: 153.8150\u001b[0m\n",
      "\u001b[32m2024-07-14 08:39:26.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mmodel saved\u001b[0m\n",
      "\u001b[32m2024-07-14 08:42:03.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mFold [5] | Epoch [1/1] | Loss: 1.1201 | Valid loss: 0.9899 | Valid mse: 2246.96948 | Valid rmse: 47.40221 | Valid mae: 28.67718 | Valid score: 0.78604 | time: 155.9186\u001b[0m\n",
      "\u001b[32m2024-07-14 08:42:03.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 18>\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mmodel saved\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda import amp # 导入AMP模块\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import math,time\n",
    "import shutil\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "criterion = LpLoss(size_average=True).cuda()  # 使用L2范数作为损失函数\n",
    "EPOCHS=300\n",
    "scaler = amp.GradScaler()\n",
    "loss_fn = valid_LpLoss(size_average=True)\n",
    "# model Constructing\n",
    "# ========================================================\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df)):\n",
    "    if os.path.exists(f\"best_valid_checkpoin_{version}-{fold}.pt\"):\n",
    "        shutil.copyfile(f\"best_valid_checkpoin_{version}-{fold}.pt\",path+f\"models/best_valid_checkpoin_{version}-{fold}.pt\")\n",
    "        continue\n",
    "    model=CustomModel()\n",
    "    model = model.cuda()\n",
    "    model=torch.compile(model)\n",
    "    ema = EMA(model, 0.999)\n",
    "    ema.register()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    train_dataset=MyDataset(train_pos[train_idx],train_press[train_idx])\n",
    "    valid_dataset=MyDataset(train_pos[valid_idx],train_press[valid_idx])\n",
    "    train_loader=DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "    valid_loader=DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    time_list=[time.time()]\n",
    "    # model Training and Saving\n",
    "    # =========================================================\n",
    "    best_score = np.inf\n",
    "    early_stop=0\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_embeds,label1 in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_embeds,label1 = input_embeds.cuda(),label1.cuda()\n",
    "            with amp.autocast():\n",
    "                modelOutput = ema.model(input_embeds)\n",
    "                loss = criterion(modelOutput, label1)\n",
    "            scaler.scale(loss).backward()    # loss缩放并反向转播\n",
    "            scaler.step(optimizer)    # 更新参数（自动unscaling）\n",
    "            scaler.update()    # 基于动态Loss Scale更新loss_scaling系数\n",
    "            epoch_loss += loss.item() * input_embeds.shape[0]\n",
    "            ema.update()\n",
    "\n",
    "        ema.apply_shadow()\n",
    "        # model Evaluating\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_preds=[]\n",
    "            valid_labels=[]\n",
    "            val_loss=0.0\n",
    "            for input_embeds,label1 in valid_loader:\n",
    "                input_embeds,label1 = input_embeds.cuda(),label1.cuda()\n",
    "                val_modelOutput = ema.model(input_embeds)\n",
    "                val_loss += criterion(val_modelOutput, label1).item()* input_embeds.shape[0]\n",
    "                valid_preds.append(val_modelOutput.cpu())\n",
    "                valid_labels.append(label1.cpu())\n",
    "\n",
    "            valid_preds=torch.cat(valid_preds,dim=0).numpy()*std+mean\n",
    "            valid_labels=torch.cat(valid_labels,dim=0).numpy()*std+mean\n",
    "            score=[]\n",
    "            for idx in range(len(valid_preds)) :\n",
    "                valid_modelOutput = valid_preds[idx]\n",
    "                label1=valid_labels[idx]\n",
    "                l2 = loss_fn(valid_modelOutput,label1)\n",
    "                score.append(l2)\n",
    "            mse = mean_squared_error(valid_labels, valid_preds)\n",
    "            rmse = math.sqrt(mse)\n",
    "            mae = mean_absolute_error(valid_labels, valid_preds)\n",
    "            score=np.array(score)\n",
    "            score=np.mean(score)\n",
    "            time_list.append(time.time())\n",
    "            logger.info(f\"Fold [{fold}] | Epoch [{epoch + 1}/{EPOCHS}] | Loss: {epoch_loss/len(train_dataset):.4f} | Valid loss: {val_loss/len(valid_dataset):.4f} | Valid mse: {mse:.5f} | Valid rmse: {rmse:.5f} | Valid mae: {mae:.5f} | Valid score: {score:.5f} | time: {time_list[-1]-time_list[-2]:.4f}\")\n",
    "\n",
    "            #  | lr : {scheduler.get_last_lr()}\n",
    "            if score < best_score:\n",
    "                # model saving\n",
    "                logger.info(\"model saved\")\n",
    "                best_score = score\n",
    "                checkpoint = {\n",
    "                    \"epoch\" : epoch,\n",
    "                    \"model_state_dict\" : ema.model.state_dict(),\n",
    "                    \"best_score\" : best_score,\n",
    "                    # \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "                    # \"scheduler_state_dict\" : scheduler.state_dict(),\n",
    "                }\n",
    "                torch.save(checkpoint, path+f\"models/best_valid_checkpoin_{version}-{fold}.pt\")\n",
    "                early_stop=0\n",
    "            else:\n",
    "                early_stop+=1\n",
    "\n",
    "        if early_stop>=50:\n",
    "            break\n",
    "        ema.restore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpwxD7ghWNa7",
    "outputId": "fb8e4696-2ed3-4d91-976b-cf64f55a1269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2268.8516 47.632463325971294 29.156158 0.77939934\n",
      "1 2722.9888 52.18226489461003 38.718784 0.85039836\n",
      "2 3020.6648 54.96057491440455 39.987633 0.90654874\n",
      "3 2187.1035 46.76647854633701 27.944115 0.77731836\n",
      "4 2691.4482 51.87916963664222 34.324905 0.85481393\n",
      "5 2246.9695 47.40220967868349 28.677177 0.7860399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import math\n",
    "fold_num=6\n",
    "kf = KFold(n_splits=fold_num, shuffle=True, random_state=42)\n",
    "oof=np.zeros(train_press.shape)\n",
    "test_preds=np.zeros((test_df.shape[0],3586))\n",
    "criterion = nn.MSELoss().cuda()\n",
    "loss_fn = valid_LpLoss(size_average=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df)):\n",
    "    model=CustomModel()\n",
    "    model=torch.compile(model)\n",
    "    state_dict=torch.load( path+f\"models/best_valid_checkpoin_{version}-{fold}.pt\")['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    model=model.cuda()\n",
    "    valid_dataset=MyDataset(train_pos[valid_idx],train_press[valid_idx])\n",
    "    valid_loader=DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_preds=[]\n",
    "        valid_labels=[]\n",
    "        val_loss=0.0\n",
    "        for input_embeds,label1 in valid_loader:\n",
    "            input_embeds,label1 = input_embeds.cuda(),label1.cuda()\n",
    "            val_modelOutput = model(input_embeds)\n",
    "            val_loss += criterion(val_modelOutput, label1).item()* input_embeds.shape[0]\n",
    "            valid_preds.append(val_modelOutput.cpu())\n",
    "            valid_labels.append(label1.cpu())\n",
    "\n",
    "        valid_preds=torch.cat(valid_preds,dim=0).numpy()*std+mean\n",
    "        valid_labels=torch.cat(valid_labels,dim=0).numpy()*std+mean\n",
    "        score=[]\n",
    "        for idx in range(len(valid_preds)) :\n",
    "            valid_modelOutput = valid_preds[idx]\n",
    "            label1=valid_labels[idx]\n",
    "            l2 = loss_fn(valid_modelOutput,label1)\n",
    "            score.append(l2)\n",
    "        mse = mean_squared_error(valid_labels, valid_preds)\n",
    "        rmse = math.sqrt(mse)\n",
    "        mae = mean_absolute_error(valid_labels, valid_preds)\n",
    "        score=np.array(score)\n",
    "        score=np.mean(score)\n",
    "        print(fold,mse,rmse,mae,score)\n",
    "        oof[valid_idx] += valid_preds\n",
    "        test_preds_tmp=[]\n",
    "        for input_embeds,label1 in test_loader:\n",
    "            input_embeds = input_embeds.cuda()\n",
    "            test_modelOutput = model(input_embeds)\n",
    "            test_preds_tmp.append(test_modelOutput.cpu())\n",
    "        test_preds_tmp=torch.cat(test_preds_tmp,dim=0).numpy()*std+mean\n",
    "        test_preds+=test_preds_tmp/fold_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ti3-9QyeWNdo",
    "outputId": "a6f62044-4fc1-4eac-8680-919df7988c6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8257096629132656"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score=[]\n",
    "for idx in range(len(oof)) :\n",
    "    valid_modelOutput = oof[idx]\n",
    "    label1=train_press[idx]*std+mean\n",
    "    l2 = loss_fn(valid_modelOutput,label1)\n",
    "    score.append(l2)\n",
    "score=np.array(score)\n",
    "score=np.mean(score)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6Z7QWfYRWNgT"
   },
   "outputs": [],
   "source": [
    "for idx,(pred) in enumerate(test_preds):\n",
    "    file_id=test_df['id'][idx]\n",
    "    np.save(path+f'submissions/content/gen_answer_A/press_{file_id}.npy',pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "slXIlsixWNjJ"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import time\n",
    "folder_path=path+f'submissions/content/gen_answer_A/'\n",
    "with zipfile.ZipFile('B_result.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, 'content/gen_answer_A/' + os.path.basename(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXnDeG8xWNmE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
